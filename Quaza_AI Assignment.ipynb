{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5a03e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Apply thresholding to enhance edges\n",
    "    _, thresholded = cv2.threshold(blurred, 50, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    return thresholded\n",
    "\n",
    "def detect_gesture(video_path, reference_path, threshold=0.1345):\n",
    "    \"\"\"\n",
    "    Detects a gesture in a video based on a reference gesture video using feature extraction and matching.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to the test video.\n",
    "        reference_path: Path to the reference gesture video.\n",
    "        threshold: Minimum ratio of good matches for gesture detection (default 0.2).\n",
    "    \"\"\"\n",
    "    # Feature extractor (SIFT)\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Load reference gesture frame\n",
    "    cap_ref = cv2.VideoCapture(reference_path)\n",
    "    if not cap_ref.isOpened():\n",
    "        print(f\"Error: Could not open reference video at {reference_path}.\")\n",
    "        return\n",
    "\n",
    "    ret, reference_frame = cap_ref.read()\n",
    "    if not ret:\n",
    "        print(f\"Error: Could not read reference frame from {reference_path}.\")\n",
    "        cap_ref.release()\n",
    "        return\n",
    "\n",
    "    reference_frame = cv2.cvtColor(reference_frame, cv2.COLOR_BGR2GRAY)\n",
    "    _, reference_des = sift.detectAndCompute(reference_frame, None)\n",
    "    cap_ref.release()\n",
    "\n",
    "    if reference_des is None:\n",
    "        print(\"Error: No features detected in the reference frame.\")\n",
    "        return\n",
    "\n",
    "    # Open test video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open test video.\")\n",
    "        return\n",
    "\n",
    "    gesture_detected = False\n",
    "    while True:\n",
    "        # Read frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Preprocess frame\n",
    "        preprocessed_frame = preprocess_frame(frame)\n",
    "\n",
    "        # Extract features from preprocessed frame\n",
    "        kp2, des2 = sift.detectAndCompute(preprocessed_frame, None)\n",
    "\n",
    "        # Matching between features using Brute-force matcher\n",
    "        bf = cv2.BFMatcher()\n",
    "        matches = bf.knnMatch(reference_des, des2, k=2)\n",
    "        good_matches = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "\n",
    "        # Check for enough good matches and set gesture_detected flag\n",
    "        gesture_detected = len(good_matches) > threshold * len(reference_des)\n",
    "\n",
    "        # Overlay \"DETECTED\" text if gesture detected\n",
    "        if gesture_detected:\n",
    "            text_origin = (frame.shape[1] - 200, 50)  # Adjust position for top right corner\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 1\n",
    "            font_thickness = 3\n",
    "            font_color = (0, 255, 0)  # Green color\n",
    "            line_type = cv2.LINE_AA\n",
    "            cv2.putText(frame, \"DETECTED\", text_origin, font, font_scale, font_color, font_thickness, line_type)\n",
    "            # Add black outline\n",
    "            cv2.putText(frame, \"DETECTED\", text_origin , font, font_scale, (0, 0, 0), 0, line_type)\n",
    "\n",
    "        # Display frame with a delay (important for visualization)\n",
    "        cv2.imshow(\"Gesture Detection\", frame)\n",
    "        cv2.waitKey(100)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "video_path = \"skip_test2.mp4\"\n",
    "reference_path = \"train_gesture.mp4\"\n",
    "detect_gesture(video_path, reference_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60892085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
